{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the different module\n",
    "import urllib2, json\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Main variable to scrap the website\n",
    "WEBSITE_URL = \"http://www.dailystormer.com/section/featured-stories/page/{0}/\"\n",
    "NUMBER_OF_PAGES = 200\n",
    "\n",
    "# Build a list of seed URL to scrap\n",
    "def build_seed_pages(url, number_of_pages_wanted):\n",
    "    seeds = []\n",
    "    for page_number in range(1, number_of_pages_wanted + 1):\n",
    "        seeds.append(url.format(page_number))\n",
    "    return seeds\n",
    "\n",
    "# Given the seeds get the pages urls\n",
    "\n",
    "def build_crawl_listing(seeds):\n",
    "    url_listing = []\n",
    "    for seed in seeds:\n",
    "        opener = urllib2.build_opener()\n",
    "        opener.addheaders = [('User-Agent', 'Mozilla/5.0')]\n",
    "        response = opener.open(seed)\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "        articles_list = soup.findAll(\"article\", { \"class\" : \"item-list\" })\n",
    "        for article in articles_list:\n",
    "            title = article.find(\"h2\")\n",
    "            if title != -1:\n",
    "                link = title.find('a')\n",
    "                url_listing.append(link.get('href'))\n",
    "    return url_listing\n",
    "\n",
    "# Extract the content of some data from a given URL\n",
    "\n",
    "def get_text_from_a_url(url):\n",
    "    opener = urllib2.build_opener()\n",
    "    opener.addheaders = [('User-Agent', 'Mozilla/5.0')]\n",
    "    response = opener.open(url)\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    article = soup.find(\"div\", { \"class\" : \"entry\" })\n",
    "    blockquotes = article.findAll('blockquote') # Remove citation\n",
    "    for blockquote in blockquotes: \n",
    "        blockquote.decompose()\n",
    "    links = article.findAll('a') # Remove link\n",
    "    for link in links:\n",
    "        link.decompose()\n",
    "    paragraphs = article.findAll(\"p\")\n",
    "    words_articles = \"\"\n",
    "    for paragraph in paragraphs[1:]:\n",
    "        words_articles = words_articles + ' ' + paragraph.text\n",
    "    return words_articles\n",
    "\n",
    "# Safe the data in a file\n",
    "\n",
    "def safe_data(json_to_dump, file_title):\n",
    "    output = json.dumps( json_to_dump, ensure_ascii=False, encoding='utf8')\n",
    "    with open(\"data/{0}.json\".format(file_title), \"w+\") as save:\n",
    "        save.write(output.encode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the crawl listing\n",
    "seeds = build_seed_pages(WEBSITE_URL, NUMBER_OF_PAGES)\n",
    "crawl_listing = build_crawl_listing(seeds)\n",
    "\n",
    "# Build the list of article\n",
    "list_of_article = []\n",
    "for url in crawl_listing:\n",
    "    list_of_article.append(get_text_from_a_url(url))\n",
    "\n",
    "# Build a dictionnary URL : Text\n",
    "result_from_crawl = {}\n",
    "for index, url in enumerate(crawl_listing):\n",
    "    result_from_crawl[url] = list_of_article[index]\n",
    "\n",
    "# Safe the dictionnary\n",
    "safe_data(result_from_crawl, 'articles')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
